{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Web Crawlers\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = 'http://en.wikipedia.org/wiki/Kevin_Bacon'\n",
    "\n",
    "try:\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(\"Http Error:\", errh)\n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print(\"Error Connecting:\", errc)\n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print(\"Timeout Error:\", errt)\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print(\"OOps: Something Else\", err)\n",
    "    \n",
    "\n",
    "html = r.text\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attrs : to extract all the tag attribute\n",
    "\n",
    "list_of_links = soup.find_all('a')\n",
    "for link in list_of_links:\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])\n",
    "        print(link.attrs, end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find('div', {'id':'bodyContent'}).find_all('a', {'href' : re.compile('^(/wiki/)((?!:).)*$')}):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><span style=\"font-family:courier\">the program defines the getLinks function, which takes in an article URL of\n",
    "the form /wiki/... , prepends the Wikipedia domain name, http://en.wikipe\n",
    "dia.org , and retrieves the BeautifulSoup object for the HTML at that domain. It\n",
    "then extracts a list of article link tags, based on the parameters discussed previously,\n",
    "and returns them.\n",
    "The main body of the program begins with setting a list of article link tags (the\n",
    "links variable) to the list of links in the initial page: https://en.wikipedia.org/wiki/\n",
    "36\n",
    "|\n",
    "Chapter 3: Writing Web CrawlersKevin_Bacon. It then goes into a loop, finding a random article link tag in the page,\n",
    "extracting the href attribute from it, printing the page, and getting a new list of links\n",
    "from the extracted URL.\n",
    "Of course, thereâ€™s a bit more to solving a Six Degrees of Wikipedia problem than\n",
    "building a scraper that goes from page to page. You must also be able to store and\n",
    "analyze the resulting data.</span><h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.core.debugger import set_trace\n",
    "\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "    r = requests.get('http://en.wikipedia.org{}'.format(articleUrl))\n",
    "    html = r.text\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    result = bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "    return result\n",
    "\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "\n",
    "\n",
    "while len(links) > 0:\n",
    "    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = set()\n",
    "\n",
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    r = requests.get('http://en.wikipedia.org{}'.format(pageUrl))\n",
    "    html = r.text\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                #We have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                print(newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)\n",
    "\n",
    "getLinks('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = set()\n",
    "\n",
    "\n",
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    r = requests.get('http://en.wikipedia.org{}'.format(pageUrl))\n",
    "    html = r.text\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    try:\n",
    "        print(bs.h1.get_text())\n",
    "        print(bs.find(id ='mw-content-text').find_all('p')[0])\n",
    "        print(bs.find(id='ca-edit').find('span').find('a').attrs['href'])\n",
    "    except AttributeError:\n",
    "        print('This page is missing something! Continuing.')\n",
    "        \n",
    "    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                #We have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                print('-'*20)\n",
    "                print(newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)\n",
    "    \n",
    "\n",
    "getLinks('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = 'http://en.wikipedia.org/wiki/Kevin_Bacon'\n",
    "\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "soup = BeautifulSoup(r.content)\n",
    "\n",
    "for a_href in soup.find_all('a', href=True):\n",
    "    print(a_href['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "pages = set()\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "\n",
    "#Retrieves a list of all Internal links found on a page\n",
    "def getInternalLinks(bs, includeUrl):\n",
    "    includeUrl = '{}://{}'.format(urlparse(includeUrl).scheme, urlparse(includeUrl).netloc)\n",
    "    internalLinks = []\n",
    "    #Finds all links that begin with a \"/\"\n",
    "    for link in bs.find_all('a', href=re.compile('^(/|.*'+includeUrl+')')):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in internalLinks:\n",
    "                if(link.attrs['href'].startswith('/')):\n",
    "                    internalLinks.append(includeUrl+link.attrs['href'])\n",
    "                else:\n",
    "                    internalLinks.append(link.attrs['href'])\n",
    "    return internalLinks\n",
    "\n",
    "\n",
    "#Retrieves a list of all external links found on a page\n",
    "def getExternalLinks(bs, excludeUrl):\n",
    "    externalLinks = []\n",
    "    #Finds all links that start with \"http\" that do\n",
    "    #not contain the current URL\n",
    "    for link in bs.find_all('a', href=re.compile('^(http|www)((?!'+excludeUrl+').)*$')):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in externalLinks:\n",
    "                externalLinks.append(link.attrs['href'])\n",
    "    return externalLinks\n",
    "\n",
    "\n",
    "def getRandomExternalLink(startingPage):\n",
    "    html = urlopen(startingPage)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    externalLinks = getExternalLinks(bs, urlparse(startingPage).netloc)\n",
    "    if len(externalLinks) == 0:\n",
    "        print('No external links, looking around the site for one')\n",
    "        domain = '{}://{}'.format(urlparse(startingPage).scheme, urlparse(startingPage).netloc)\n",
    "        internalLinks = getInternalLinks(bs, domain)\n",
    "        return getRandomExternalLink(internalLinks[random.randint(0, len(internalLinks)-1)])\n",
    "    else:\n",
    "        return externalLinks[random.randint(0, len(externalLinks)-1)]\n",
    "\n",
    "\n",
    "def followExternalOnly(startingSite):\n",
    "    externalLink = getRandomExternalLink(startingSite)\n",
    "    print('Random external link is: {}'.format(externalLink))\n",
    "    followExternalOnly(externalLink)\n",
    "\n",
    "followExternalOnly('http://oreilly.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects a list of all external URLs found on the site\n",
    "allExtLinks = set()\n",
    "allIntLinks = set()\n",
    "\n",
    "\n",
    "def getAllExternalLinks(siteUrl):\n",
    "    html = urlopen(siteUrl)\n",
    "    domain = '{}://{}'.format(urlparse(siteUrl).scheme,\n",
    "    urlparse(siteUrl).netloc)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    internalLinks = getInternalLinks(bs, domain)\n",
    "    externalLinks = getExternalLinks(bs, domain)\n",
    "    for link in externalLinks:\n",
    "        if link not in allExtLinks:\n",
    "            allExtLinks.add(link)\n",
    "            print(link)\n",
    "    for link in internalLinks:\n",
    "        if link not in allIntLinks:\n",
    "            allIntLinks.add(link)\n",
    "            getAllExternalLinks(link)\n",
    "allIntLinks.add('http://oreilly.com')\n",
    "getAllExternalLinks('http://oreilly.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.85px",
    "left": "1569px",
    "right": "20px",
    "top": "4px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
